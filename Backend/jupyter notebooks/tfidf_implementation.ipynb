{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1161c5bf",
   "metadata": {},
   "source": [
    "## **CONSTANTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40f5891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"./data/simplewiki-latest-pages-articles-multistream.xml.bz2\"\n",
    "CSV_OUTPUT = \"./data/simplewiki_articles.csv\"\n",
    "PROCESSED_TOKENS_OUTPUT = \"./data/dataset_with_processed_tokens.jsonl\"\n",
    "INVERTED_INDEX_FILE = \"./data/inverted_index.pkl\"\n",
    "INVERSE_DOCUMENT_FREQUENCY_FILE = \"./data/inverse_document_frequency.pkl\"\n",
    "SQL_DATABASE_FILENAME = './db/wikipedia_snippets.db'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c469e5",
   "metadata": {},
   "source": [
    "## **Utility Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e62ec92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pympler import asizeof\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b92c871",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\baigj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\baigj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e810e650",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEMMER = PorterStemmer()\n",
    "STOP_WORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55d5943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_consumed(obj):\n",
    "    size_bytes = asizeof.asizeof(obj)\n",
    "    size_mb = size_bytes / (1024 * 1024)\n",
    "    print(f\"{size_mb:.2f} MB\")\n",
    "    \n",
    "    return size_mb\n",
    "\n",
    "def preprocess_text(text) -> list[str]:\n",
    "    \n",
    "    # Step 1: Normalize the text to keep only alphanumeric text and single space instead of multiple spaces.\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Step 2: Tokenize the entire text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Step 3: For each token -> filter out stopwords and tokens with only 1 character, lowercase, and stem to base form\n",
    "    processed_tokens = [\n",
    "        STEMMER.stem(token)\n",
    "        for token in tokens\n",
    "        if (token not in STOP_WORDS and len(token) > 1)\n",
    "    ]\n",
    "    \n",
    "    # Step 4: Return the processed tokens\n",
    "    return processed_tokens\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02df9535",
   "metadata": {},
   "source": [
    "## **TF-IDF Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d10a244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sqlite3\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d9ea12",
   "metadata": {},
   "source": [
    "#### **Load the stored inverted index and inverse document frequency**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d913044",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(INVERTED_INDEX_FILE, \"rb\") as f:\n",
    "    inverted_index = pickle.load(f)\n",
    "\n",
    "with open(INVERSE_DOCUMENT_FREQUENCY_FILE, \"rb\") as f:\n",
    "    idf_data = pickle.load(f)\n",
    "    \n",
    "idf = idf_data['idf']\n",
    "total_doc_count = idf_data['total_documents']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db94fe25",
   "metadata": {},
   "source": [
    "**Function to get PageIds from Inverted Indexes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e44fc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_and_retrieve_pages_with_tfidf(query, top_k=20):\n",
    "\n",
    "    query = query.lower()\n",
    "    processed_tokens = preprocess_text(query)\n",
    "\n",
    "    scores = defaultdict(float)\n",
    "\n",
    "    for term in processed_tokens:\n",
    "        if term not in inverted_index:\n",
    "            continue\n",
    "\n",
    "        postings = inverted_index[term] # -> {pageId: tf}\n",
    "        term_idf = idf.get(term, 0.0)\n",
    "\n",
    "        for page_id, tf in postings.items():\n",
    "            scores[page_id] += tf * term_idf\n",
    "\n",
    "    ranked_docs = sorted(\n",
    "        scores.items(),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    return [page_id for page_id, score in ranked_docs[:top_k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "51b07c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_query(query):\n",
    "    # Step 1: Get ranked pageIds from TFâ€“IDF\n",
    "    pageIds = rank_and_retrieve_pages_with_tfidf(query, top_k=20)\n",
    "    if not pageIds:\n",
    "        return []\n",
    "\n",
    "    # Step 2: Preprocess query tokens for matching\n",
    "    \n",
    "    query_tokens = preprocess_text(query)\n",
    "\n",
    "    # Step 3: Connect to DB and fetch metadata\n",
    "    with sqlite3.connect(SQL_DATABASE_FILENAME) as conn:\n",
    "        cursor = conn.cursor()\n",
    "        placeholders = ','.join(['?'] * len(pageIds))\n",
    "        sql = f\"SELECT PageId, Title, Snippet, URL FROM articles WHERE PageId IN ({placeholders})\"\n",
    "        cursor.execute(sql, pageIds)\n",
    "        results = cursor.fetchall()\n",
    "\n",
    "    # Step 4: Build lookup dict for ordering\n",
    "    results_dict = {row[0]: row for row in results}\n",
    "    docs = [results_dict[pid] for pid in pageIds if pid in results_dict]\n",
    "\n",
    "    # Step 5: Re-rank based on title match\n",
    "    def title_match_score(doc):\n",
    "        title = doc[1].lower()\n",
    "        # Count query tokens that appear in the title\n",
    "        return sum(1 for token in query_tokens if token in title)\n",
    "\n",
    "    # Apply re-ranking: original order + title match boost\n",
    "    docs.sort(key=lambda doc: (title_match_score(doc), pageIds.index(doc[0])), reverse=True)\n",
    "\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3a03de5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('37039',\n",
       "  'Model',\n",
       "  'Model, models, or modeling can mean: An abstract (idea, theory, simulation) or smaller approximation of an object or system for testing. (Like a car, building or ship.)\\n\\nIdeas, concepts or software\\n Business model\\n Model (abstract), an abstract or conceptual object used in the creation of math to predict its behavior\\n Causal model\\n Mathematical model\\n Scientific model\\n Model Driven Engineering, a software development technique based on abstract models\\n Metamodeling, a model of a model\\n Molecular',\n",
       "  'https://simple.wikipedia.org/wiki/Model'),\n",
       " ('218',\n",
       "  'Earth science',\n",
       "  \"thumb|250px|A volcano eruption is the release of stored energy from below the surface of Earth. The heat comes mostly from radioactive decay, and convection, in the Earth's core and mantle.Encyclopedia of Volcanoes. Academic Press, London, 2000.\\n\\nEarth science is an all-covering term for the sciences related to the planet Earth. Earth science may also be called geoscience. Geoscience is the study of the architecture of the Earth.\\n\\nIt is a broader term than geology because it includes aspects of \",\n",
       "  'https://simple.wikipedia.org/wiki/Earth_science')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_query(\"Deep learning models in healthcare\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1218f265",
   "metadata": {},
   "source": [
    "## **Free Memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "33618f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "274"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "del inverted_index\n",
    "del idf_data\n",
    "del idf\n",
    "\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
