{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1161c5bf",
   "metadata": {},
   "source": [
    "## **CONSTANTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40f5891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"./data/simplewiki-latest-pages-articles-multistream.xml.bz2\"\n",
    "CSV_OUTPUT = \"./data/simplewiki_articles.csv\"\n",
    "PROCESSED_TOKENS_OUTPUT = \"./data/dataset_with_processed_tokens.jsonl\"\n",
    "INVERTED_INDEX_FILE = \"./data/inverted_index.pkl\"\n",
    "INVERSE_DOCUMENT_FREQUENCY_FILE = \"./data/inverse_document_frequency.pkl\"\n",
    "SQL_DATABASE_FILENAME = './db/wikipedia_snippets.db'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c469e5",
   "metadata": {},
   "source": [
    "## **Utility Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e62ec92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pympler import asizeof\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b92c871",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\baigj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\baigj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e810e650",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEMMER = PorterStemmer()\n",
    "STOP_WORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f55d5943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_consumed(obj):\n",
    "    size_bytes = asizeof.asizeof(obj)\n",
    "    size_mb = size_bytes / (1024 * 1024)\n",
    "    print(f\"{size_mb:.2f} MB\")\n",
    "    \n",
    "    return size_mb\n",
    "\n",
    "def generate_url(title):\n",
    "    return f\"https://simple.wikipedia.org/wiki/{title.replace(\" \", \"_\")}\"\n",
    "\n",
    "def preprocess_text(text) -> list[str]:\n",
    "    \n",
    "    # Step 1: Normalize the text to keep only alphanumeric text and single space instead of multiple spaces.\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Step 2: Tokenize the entire text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Step 3: For each token -> filter out stopwords and tokens with only 1 character, lowercase, and stem to base form\n",
    "    processed_tokens = [\n",
    "        STEMMER.stem(token)\n",
    "        for token in tokens\n",
    "        if (token not in STOP_WORDS and len(token) > 1)\n",
    "    ]\n",
    "    \n",
    "    # Step 4: Return the processed tokens\n",
    "    return processed_tokens\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e0c232",
   "metadata": {},
   "source": [
    "## **Read and Parse the XML content and save into csv file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "191f7241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import mwxml\n",
    "import mwparserfromhell as mwp\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "909c7bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 278900 articles...\n",
      "Done! Saved 278950 articles to 'simplewiki_articles.csv'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(CSV_OUTPUT, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['PageId', 'Title', 'Text'])\n",
    "\n",
    "    with bz2.open(FILE_PATH, 'rt', encoding='utf-8') as f:\n",
    "        dump = mwxml.Dump.from_file(f)\n",
    "\n",
    "        count = 0\n",
    "        for page in dump:\n",
    "            \n",
    "            if page.namespace != 0 or page.redirect:\n",
    "                continue\n",
    "\n",
    "            for revision in page:\n",
    "                if revision.text:\n",
    "                    try:\n",
    "                        wikicode = mwp.parse(revision.text)\n",
    "                        plain_text = wikicode.strip_code().strip()\n",
    "\n",
    "                        if plain_text:\n",
    "                            writer.writerow([page.id, page.title, plain_text])\n",
    "                            count += 1\n",
    "                            if count % 100 == 0:\n",
    "                                print(f\"Saved {count} articles...\", end='\\r')\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error parsing page {page.title}: {e}\")\n",
    "\n",
    "                break \n",
    "\n",
    "print(f\"\\nDone! Saved {count} articles to '{CSV_OUTPUT}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feab71f",
   "metadata": {},
   "source": [
    "## **Text Normalization and Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69f10403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1001487b",
   "metadata": {},
   "source": [
    "##### **Loading CSV that contains parsed articles aata**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c451503c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baigj\\AppData\\Local\\Temp\\ipykernel_14316\\2839121091.py:1: DtypeWarning: Columns (0: PageId, 1: Unnamed: 3, 2: Unnamed: 4, 3: Unnamed: 5, 4: Unnamed: 6, 5: Unnamed: 7, 6: Unnamed: 8, 7: Unnamed: 9, 8: Unnamed: 10, 9: Unnamed: 11, 10: Unnamed: 12, 11: Unnamed: 13, 12: Unnamed: 14, 13: Unnamed: 15, 14: Unnamed: 16, 15: Unnamed: 17, 16: Unnamed: 18, 17: Unnamed: 19, 18: Unnamed: 20, 19: Unnamed: 21, 20: Unnamed: 22, 21: Unnamed: 23, 22: Unnamed: 24, 23: Unnamed: 25, 24: Unnamed: 26, 25: Unnamed: 27, 26: Unnamed: 28, 27: Unnamed: 29, 28: Unnamed: 30, 29: Unnamed: 31, 30: Unnamed: 32, 31: Unnamed: 33, 32: Unnamed: 34, 33: Unnamed: 35, 34: Unnamed: 36, 35: Unnamed: 37, 36: Unnamed: 38, 37: Unnamed: 39, 38: Unnamed: 40, 39: Unnamed: 41, 40: Unnamed: 42, 41: Unnamed: 43, 42: Unnamed: 44, 43: Unnamed: 45, 44: Unnamed: 46, 45: Unnamed: 47, 46: Unnamed: 48, 47: Unnamed: 49, 48: Unnamed: 50, 49: Unnamed: 51, 50: Unnamed: 52, 51: Unnamed: 53, 52: Unnamed: 54, 53: Unnamed: 55, 54: Unnamed: 56, 55: Unnamed: 57, 56: Unnamed: 58, 57: Unnamed: 59, 58: Unnamed: 60, 59: Unnamed: 61, 60: Unnamed: 62, 61: Unnamed: 63, 62: Unnamed: 64, 63: Unnamed: 65, 64: Unnamed: 66, 65: Unnamed: 67, 66: Unnamed: 68, 67: Unnamed: 69, 68: Unnamed: 70, 69: Unnamed: 71, 70: Unnamed: 72, 71: Unnamed: 73, 72: Unnamed: 74, 73: Unnamed: 75, 74: Unnamed: 76, 75: Unnamed: 77, 76: Unnamed: 78, 77: Unnamed: 79, 78: Unnamed: 80, 79: Unnamed: 81, 80: Unnamed: 82, 81: Unnamed: 83, 82: Unnamed: 84, 83: Unnamed: 85, 84: Unnamed: 86, 85: Unnamed: 87, 86: Unnamed: 88, 87: Unnamed: 89, 88: Unnamed: 90, 89: Unnamed: 91, 90: Unnamed: 92, 91: Unnamed: 93, 92: Unnamed: 94, 93: Unnamed: 95, 94: Unnamed: 96, 95: Unnamed: 97, 96: Unnamed: 98, 97: Unnamed: 99, 98: Unnamed: 100, 99: Unnamed: 101, 100: Unnamed: 102, 101: Unnamed: 103, 102: Unnamed: 104, 103: Unnamed: 105, 104: Unnamed: 106, 105: Unnamed: 107, 106: Unnamed: 108, 107: Unnamed: 109, 108: Unnamed: 110, 109: Unnamed: 112, 110: Unnamed: 113, 111: Unnamed: 114, 112: Unnamed: 115, 113: Unnamed: 116, 114: Unnamed: 117, 115: Unnamed: 118, 116: Unnamed: 119, 117: Unnamed: 120, 118: Unnamed: 121, 119: Unnamed: 122, 120: Unnamed: 123, 121: Unnamed: 124, 122: Unnamed: 125, 123: Unnamed: 126, 124: Unnamed: 127, 125: Unnamed: 128, 126: Unnamed: 129, 127: Unnamed: 130, 128: Unnamed: 131, 129: Unnamed: 132, 130: Unnamed: 133, 131: Unnamed: 134, 132: Unnamed: 135, 133: Unnamed: 136, 134: Unnamed: 137, 135: Unnamed: 138, 136: Unnamed: 139, 137: Unnamed: 140, 138: Unnamed: 141, 139: Unnamed: 142, 140: Unnamed: 143, 141: Unnamed: 144, 142: Unnamed: 145, 143: Unnamed: 146, 144: Unnamed: 147, 145: Unnamed: 148, 146: Unnamed: 149, 147: Unnamed: 150, 148: Unnamed: 151, 149: Unnamed: 152, 150: Unnamed: 153, 151: Unnamed: 154, 152: Unnamed: 155, 153: Unnamed: 156, 154: Unnamed: 157, 155: Unnamed: 158, 156: Unnamed: 159, 157: Unnamed: 160, 158: Unnamed: 161, 159: Unnamed: 162, 160: Unnamed: 163, 161: Unnamed: 164, 162: Unnamed: 166, 163: Unnamed: 167, 164: Unnamed: 168, 165: Unnamed: 169, 166: Unnamed: 170, 167: Unnamed: 171, 168: Unnamed: 172, 169: Unnamed: 173, 170: Unnamed: 174, 171: Unnamed: 175, 172: Unnamed: 176, 173: Unnamed: 177, 174: Unnamed: 178, 175: Unnamed: 179, 176: Unnamed: 180, 177: Unnamed: 181, 178: Unnamed: 182, 179: Unnamed: 183, 180: Unnamed: 184, 181: Unnamed: 185, 182: Unnamed: 186, 183: Unnamed: 187, 184: Unnamed: 188, 185: Unnamed: 189, 186: Unnamed: 190, 187: Unnamed: 191, 188: Unnamed: 192, 189: Unnamed: 193, 190: Unnamed: 194, 191: Unnamed: 195, 192: Unnamed: 196, 193: Unnamed: 197, 194: Unnamed: 198, 195: Unnamed: 199, 196: Unnamed: 200, 197: Unnamed: 201, 198: Unnamed: 202, 199: Unnamed: 203, 200: Unnamed: 204, 201: Unnamed: 205, 202: Unnamed: 206, 203: Unnamed: 207, 204: Unnamed: 208, 205: Unnamed: 209, 206: Unnamed: 210, 207: Unnamed: 211, 208: Unnamed: 212, 209: Unnamed: 213, 210: Unnamed: 214, 211: Unnamed: 215, 212: Unnamed: 216, 213: Unnamed: 217, 214: Unnamed: 218, 215: Unnamed: 219, 216: Unnamed: 220, 217: Unnamed: 221, 218: Unnamed: 222, 219: Unnamed: 223, 220: Unnamed: 224, 221: Unnamed: 225, 222: Unnamed: 226, 223: Unnamed: 227, 224: Unnamed: 228, 225: Unnamed: 229, 226: Unnamed: 230, 227: Unnamed: 231, 228: Unnamed: 232, 229: Unnamed: 233, 230: Unnamed: 234, 231: Unnamed: 235, 232: Unnamed: 236, 233: Unnamed: 237, 234: Unnamed: 238, 235: Unnamed: 239, 236: Unnamed: 240, 237: Unnamed: 241, 238: Unnamed: 242, 239: Unnamed: 243, 240: Unnamed: 244, 241: Unnamed: 245, 242: Unnamed: 246, 243: Unnamed: 247, 244: Unnamed: 248, 245: Unnamed: 249, 246: Unnamed: 250, 247: Unnamed: 251, 248: Unnamed: 252, 249: Unnamed: 253, 250: Unnamed: 254, 251: Unnamed: 255, 252: Unnamed: 256, 253: Unnamed: 257, 254: Unnamed: 258, 255: Unnamed: 259, 256: Unnamed: 260, 257: Unnamed: 261, 258: Unnamed: 262, 259: Unnamed: 263, 260: Unnamed: 264, 261: Unnamed: 265, 262: Unnamed: 266, 263: Unnamed: 267, 264: Unnamed: 268, 265: Unnamed: 269, 266: Unnamed: 270, 267: Unnamed: 271, 268: Unnamed: 272, 269: Unnamed: 273, 270: Unnamed: 274, 271: Unnamed: 275, 272: Unnamed: 276, 273: Unnamed: 277, 274: Unnamed: 278, 275: Unnamed: 279, 276: Unnamed: 280, 277: Unnamed: 281, 278: Unnamed: 282, 279: Unnamed: 283, 280: Unnamed: 284, 281: Unnamed: 285, 282: Unnamed: 286, 283: Unnamed: 287, 284: Unnamed: 288, 285: Unnamed: 289, 286: Unnamed: 290, 287: Unnamed: 291, 288: Unnamed: 292, 289: Unnamed: 293, 290: Unnamed: 294, 291: Unnamed: 295, 292: Unnamed: 296, 293: Unnamed: 297, 294: Unnamed: 298, 295: Unnamed: 299, 296: Unnamed: 300, 297: Unnamed: 301, 298: Unnamed: 302, 299: Unnamed: 303, 300: Unnamed: 304, 301: Unnamed: 305, 302: Unnamed: 306, 303: Unnamed: 307, 304: Unnamed: 308, 305: Unnamed: 309, 306: Unnamed: 310, 307: Unnamed: 311, 308: Unnamed: 312, 309: Unnamed: 313, 310: Unnamed: 314, 311: Unnamed: 315, 312: Unnamed: 316, 313: Unnamed: 317, 314: Unnamed: 318, 315: Unnamed: 319, 316: Unnamed: 320, 317: Unnamed: 321, 318: Unnamed: 322, 319: Unnamed: 323, 320: Unnamed: 324, 321: Unnamed: 325, 322: Unnamed: 326, 323: Unnamed: 327, 324: Unnamed: 328, 325: Unnamed: 329, 326: Unnamed: 330, 327: Unnamed: 331, 328: Unnamed: 332, 329: Unnamed: 333, 330: Unnamed: 334, 331: Unnamed: 335, 332: Unnamed: 336, 333: Unnamed: 337, 334: Unnamed: 338, 335: Unnamed: 339) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataset = pd.read_csv(CSV_OUTPUT)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PageId</th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>April</td>\n",
       "      <td>April (Apr.) is the fourth month of the year i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>August</td>\n",
       "      <td>August (Aug.) is the eighth month of the year ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>Art</td>\n",
       "      <td>thumb|300x300px|A painting by Renoir is a work...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>A</td>\n",
       "      <td>thumb|Writing \"A\" in cursive font.\\n\\nA is the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>Air</td>\n",
       "      <td>thumb|A fan moves air.\\n\\nAir is the Earth's a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PageId   Title                                               Text\n",
       "0      1   April  April (Apr.) is the fourth month of the year i...\n",
       "1      2  August  August (Aug.) is the eighth month of the year ...\n",
       "2      6     Art  thumb|300x300px|A painting by Renoir is a work...\n",
       "3      8       A  thumb|Writing \"A\" in cursive font.\\n\\nA is the...\n",
       "4      9     Air  thumb|A fan moves air.\\n\\nAir is the Earth's a..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(CSV_OUTPUT)\n",
    "\n",
    "dataset = dataset[[\"PageId\", \"Title\", \"Text\"]]\n",
    "dataset.dropna(inplace=True)\n",
    "dataset.drop_duplicates(subset=[\"PageId\"], inplace=True)\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561b52f1",
   "metadata": {},
   "source": [
    "**Save this data into a SQLite databse. It will be used in query time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e529df81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PageId</th>\n",
       "      <th>Title</th>\n",
       "      <th>Snippet</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>April</td>\n",
       "      <td>April (Apr.) is the fourth month of the year i...</td>\n",
       "      <td>https://simple.wikipedia.org/wiki/April</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>August</td>\n",
       "      <td>August (Aug.) is the eighth month of the year ...</td>\n",
       "      <td>https://simple.wikipedia.org/wiki/August</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>Art</td>\n",
       "      <td>thumb|300x300px|A painting by Renoir is a work...</td>\n",
       "      <td>https://simple.wikipedia.org/wiki/Art</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PageId   Title                                            Snippet  \\\n",
       "0      1   April  April (Apr.) is the fourth month of the year i...   \n",
       "1      2  August  August (Aug.) is the eighth month of the year ...   \n",
       "2      6     Art  thumb|300x300px|A painting by Renoir is a work...   \n",
       "\n",
       "                                        URL  \n",
       "0   https://simple.wikipedia.org/wiki/April  \n",
       "1  https://simple.wikipedia.org/wiki/August  \n",
       "2     https://simple.wikipedia.org/wiki/Art  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Extract a short snippet of the original text and create a URL to link back to original source of article\n",
    "dataset['Snippet'] = dataset['Text'].str[:500]\n",
    "dataset['URL'] = dataset['Title'].apply(generate_url)\n",
    "\n",
    "# Step 2: Extract the columns that are required to be stored in the SQLite database\n",
    "dataset_to_store = dataset[['PageId', 'Title', 'Snippet', \"URL\"]]\n",
    "dataset_to_store.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dcddbc98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Create a connection with the SQLite database\n",
    "conn = sqlite3.connect(SQL_DATABASE_FILENAME)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Step 4: Create an empty table named articles\n",
    "cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS articles (\n",
    "        PageId INTEGER PRIMARY KEY,\n",
    "        Title TEXT,\n",
    "        Snippet TEXT,\n",
    "        URL TEXT\n",
    "    )\n",
    "''')\n",
    "# Step 4: Save the data into `articles` table\n",
    "dataset_to_store.to_sql('articles', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Step 5: Commit the changes and close the connection\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "# Step 6: After this, dataset_to_store will be consuming almost 258.33 MB of memory useless. Free it.\n",
    "del dataset_to_store\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199aa2e1",
   "metadata": {},
   "source": [
    "##### **Apply proprocessing on text and save it in a jsonl file for safe future use**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9074eaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f5dfcb39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles: 100%|██████████| 279461/279461 [18:36<00:00, 250.23it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(PROCESSED_TOKENS_OUTPUT, \"w\", encoding=\"utf-8\") as file:\n",
    "\n",
    "    for row in tqdm(dataset.itertuples(), total=len(dataset), desc=\"Processing articles\"):\n",
    "        \n",
    "        # Filter invalid PageIds\n",
    "        if str(row.PageId).isnumeric():\n",
    "            record = {\n",
    "                \"PageId\": row.PageId,\n",
    "                \"Processed_Tokens\": preprocess_text(row.Text),\n",
    "            }\n",
    "\n",
    "            file.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6daeb36",
   "metadata": {},
   "source": [
    "##### **Free Non required memory**\n",
    "\n",
    "After this step, we don't need the actual dataset DataFrame. We have stored its contents in a faster SQLite database and stored the processed tokens in the jsonl file for future processing if required.\n",
    "\n",
    "Memory consumed by dataset variable = >800 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c555d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del dataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1726d4fa",
   "metadata": {},
   "source": [
    "## **Creation of Inverted Indexes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a58bdb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import math\n",
    "import gc\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78b732de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building inverted index and collecting metadata for inverse document frequency.: 100%|██████████| 278941/278941 [00:28<00:00, 9829.28it/s] \n",
      "Computing IDF: 100%|██████████| 877202/877202 [00:02<00:00, 335595.92it/s]\n"
     ]
    }
   ],
   "source": [
    "inverted_index = defaultdict(dict)\n",
    "total_doc_count = 0  \n",
    "idf = {}\n",
    "\n",
    "# For BM25 implementation\n",
    "doc_lengths = {}\n",
    "avg_doc_length = 0\n",
    "\n",
    "\n",
    "# Count total lines once (for proper progress bar)\n",
    "with open(PROCESSED_TOKENS_OUTPUT, \"r\", encoding=\"utf-8\") as f:\n",
    "    total_lines = sum(1 for _ in f)\n",
    "\n",
    "\n",
    "with open(PROCESSED_TOKENS_OUTPUT, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f, total=total_lines, desc=\"Building inverted index and collecting metadata for inverse document frequency.\"):\n",
    "        data = json.loads(line)\n",
    "        page_id = data[\"PageId\"]\n",
    "        tokens = data[\"Processed_Tokens\"]\n",
    "\n",
    "        total_doc_count += 1\n",
    "        \n",
    "        doc_length = len(tokens)\n",
    "        doc_lengths[page_id] = doc_length\n",
    "        avg_doc_length += doc_length\n",
    "\n",
    "        term_counts = Counter(tokens)\n",
    "\n",
    "        for term, tf in term_counts.items():\n",
    "            inverted_index[term][page_id] = tf\n",
    "            \n",
    "avg_doc_length = avg_doc_length / total_doc_count\n",
    "\n",
    "\n",
    "for term, postings in tqdm(\n",
    "        inverted_index.items(),\n",
    "        total=len(inverted_index),\n",
    "        desc=\"Computing IDF\"\n",
    "    ):\n",
    "    \n",
    "    document_frequency = len(postings)\n",
    "\n",
    "    idf[term] = np.float32(\n",
    "        math.log((total_doc_count + 1) / (document_frequency + 1)) + 1\n",
    "    )\n",
    "\n",
    "\n",
    "with open(INVERTED_INDEX_FILE, \"wb\") as f:\n",
    "    pickle.dump(inverted_index, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "with open(INVERSE_DOCUMENT_FREQUENCY_FILE, \"wb\") as f:\n",
    "    pickle.dump(\n",
    "        {\n",
    "            \"total_documents\": total_doc_count,\n",
    "            \"doc_lengths\": doc_lengths,\n",
    "            \"avg_doc_length\": avg_doc_length,\n",
    "            \"idf\": idf\n",
    "        },\n",
    "        f,\n",
    "        protocol=pickle.HIGHEST_PROTOCOL\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b839d82a",
   "metadata": {},
   "source": [
    "##### **Free Non required memory**\n",
    "\n",
    "After this step, we don't need the actual inverted_index, total_doc_count, and idf variable.\n",
    "\n",
    "Memory consumed by inverted_index variable = >1 GB\n",
    "\n",
    "Memory consumed by total_doc_count variable = 0 MB\n",
    "\n",
    "Memory consumed by idf variable = 99.54 MB\n",
    "\n",
    "Memory consumed by avg_doc_length variable = 0 MB\n",
    "\n",
    "Memory consumed by doc_lengths variable = 19.87 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efb1e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del inverted_index\n",
    "del total_doc_count\n",
    "del doc_lengths\n",
    "del avg_doc_length\n",
    "del idf\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a4362b",
   "metadata": {},
   "source": [
    "After this Step, Weh have successfully parsed the XML, Preprocessed the text, and created an efficient inverted index along with term frequencies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
